{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SchNet on the JClinic dataset\n",
    "\n",
    "> Train a simple SchNet model to predict the JClinic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from prody import parsePDB\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from jclinic.data import create_raw_dataset, JClinicDataset, make_train_val_split_clustering_by_rmsd\n",
    "from jclinic.models import SchNet\n",
    "from jclinic.pairwise_rmsd import make_rmsds_matrix\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the logging module to store output in a log file for easy reference while printing it to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./tmp_out/jclinic/log', exist_ok=True)\n",
    "logger = logging.getLogger('Training a SchNet model with vanilla PyTorch')\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "timeticks = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "file_handler = logging.FileHandler(\n",
    "    os.path.join('./tmp_out/jclinic/log', f'{timeticks}.log'))\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random number seed in all modules to guarantee the same result when running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_dir = \"../data/structures\"\n",
    "fixed_structures_dir = \"../data/structures_fixed\"\n",
    "pretrained_esm_model = \"esm2_t12_35M_UR50D\"\n",
    "embeddings_dir = f\"../data/embeddings_fixed_{pretrained_esm_model}\"\n",
    "labels_path = \"../data/labels.txt\"\n",
    "\n",
    "dataset_dir = f\"../data/pyg_dataset_{pretrained_esm_model}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the PDB files using the scripts in `jclinic.fix_pdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m jclinic.fix_pdb $structures_dir $fixed_structures_dir --rename_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the AA sequences and compute their ESM-2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m jclinic.sequence_embeddings $fixed_structures_dir $embeddings_dir --pretrained_esm_model=$pretrained_esm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an `InMemoryDataset` instance for PyTorch Geometric\n",
    "\n",
    "The dataset contains the 3D coordinates of all Ca atoms (`pos`), the per-residue ESM-2 embeddings (`esm_embeddings`), and the target labels for prediction (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_raw_dataset(\n",
    "    fixed_structures_dir,\n",
    "    embeddings_dir,\n",
    "    labels_path,\n",
    "    dataset_dir\n",
    ")\n",
    "dataset = JClinicDataset(dataset_dir)\n",
    "\n",
    "esm_embedding_dim = dataset.esm_embeddings.shape[-1]\n",
    "print(f\"Dimension of ESM-2 embeddings = {esm_embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a \"hard\" train-validation split to avoid data leakage\n",
    "\n",
    "Structures in the validation set should not be too close in 3D structure (and sequence) to those in the training set, to avoid inflating results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_structures_prody = {\n",
    "    data.name: parsePDB(f\"{fixed_structures_dir}/{data.name}.pdb\") for data in dataset\n",
    "}\n",
    "\n",
    "rmsds_matrix = make_rmsds_matrix(parsed_structures_prody)\n",
    "rmsds_matrix_finite = rmsds_matrix.copy()\n",
    "rmsds_matrix_finite[rmsds_matrix_finite.isna()] = 2 * rmsds_matrix_finite.max(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we cluster structures using complete linkage according to their pairwise RMSD (if available), and using a distance cutoff. Then, each cluster is either included entirely in the training set or entirely in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_cutoff = 8\n",
    "train_frac = 0.8\n",
    "\n",
    "train_idxs, val_idxs = make_train_val_split_clustering_by_rmsd(\n",
    "    rmsds_matrix_finite, clustering_cutoff, train_frac=train_frac\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.copy()\n",
    "train_dataset = train_dataset[train_idxs]\n",
    "\n",
    "val_dataset = dataset.copy()\n",
    "val_dataset = val_dataset[val_idxs]\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader, validation_loader, model, loss_fn, optimizer, epochs=50\n",
    "):\n",
    "    size_train = len(train_loader.dataset)\n",
    "    size_val = len(validation_loader.dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        num_batches = len(train_loader)\n",
    "        train_loss = 0\n",
    "        for data in train_loader:\n",
    "            n_samples = len(data)\n",
    "            pos = data.pos.to(device)\n",
    "            esm_embeddings = data.esm_embeddings.to(device)\n",
    "            batch = data.batch\n",
    "            y = data.y.to(device)\n",
    "    \n",
    "            # Compute prediction error\n",
    "            pred = model(esm_embeddings, pos, batch)\n",
    "            # print(f\"Train: {pred}, {y}\")\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss += loss.item() * n_samples\n",
    "    \n",
    "            # Backpropagate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= size_train\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                n_samples = len(data)\n",
    "                pos = data.pos.to(device)\n",
    "                esm_embeddings = data.esm_embeddings.to(device)\n",
    "                batch = data.batch\n",
    "                y = data.y.to(device)\n",
    "    \n",
    "                pred = model(esm_embeddings, pos, batch)\n",
    "                # print(f\"Val: {pred}, {y}\")\n",
    "                val_loss += loss_fn(pred, y).item() * n_samples\n",
    "        val_loss /= size_val\n",
    "\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch}: Training error = {math.sqrt(train_loss):.3f}, \"\n",
    "            f\"Validation error = {math.sqrt(val_loss):.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and train\n",
    "\n",
    "Given the small dataset size, we set hyperparamters using the following considerations:\n",
    "1. The number of hidden channels in the `SchNet` should be smaller than the default of 128 to avoid overparametrization and overfitting\n",
    "2. Ditto as above for the number of filters in the `SchNet`\n",
    "3. Possibly ditto as above for the number of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "hidden_channels = 1\n",
    "num_filters = 2\n",
    "num_interactions = 2\n",
    "num_gaussians = 50\n",
    "atom_distance_cutoff = 20\n",
    "readout = \"sum\"  # This is crucial\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = SchNet(\n",
    "    esm_embedding_dim=esm_embedding_dim,\n",
    "    hidden_channels=hidden_channels,\n",
    "    num_filters=num_filters,\n",
    "    num_interactions=num_interactions,\n",
    "    num_gaussians=num_gaussians,\n",
    "    cutoff=atom_distance_cutoff,\n",
    "    readout=readout,\n",
    ").to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {total_params}\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=1e-3, weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, val_loader, model, loss_fn, optimizer, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
