{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence_embeddings\n",
    "\n",
    "> Compute sequence embeddings using ESM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Union, Optional\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prody import AtomGroup, parsePDB\n",
    "\n",
    "import torch\n",
    "\n",
    "import esm\n",
    "\n",
    "from fastcore.script import *\n",
    "\n",
    "PRETRAINED_ESM_MODELS = [x for x in dir(esm.pretrained) if x.startswith(\"esm2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_sequences_from_parsed_prody(parsed_structures_prody: dict[str, AtomGroup]):\n",
    "    \"\"\"Extract amino acid sequences from parsed ProDy structures using Ca atoms.\"\"\"\n",
    "    sequences = {}\n",
    "    for name, structure in parsed_structures_prody.items():\n",
    "        sequences[name] = {}\n",
    "        for chain in structure.getHierView():\n",
    "            chain_id = chain.getChid()\n",
    "            chain_ca = chain.select(\"name CA\")\n",
    "            chain_seq = chain_ca.getSequence()\n",
    "            sequences[name][chain_id] = chain_seq\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def embeddings_from_parsed_prody(\n",
    "    parsed_structures_prody: dict[str, AtomGroup],\n",
    "    pretrained_esm_model: str = \"esm2_t36_3B_UR50D\",\n",
    "    repr_layers: Optional[list[int]] = None,\n",
    "):\n",
    "    \"\"\"Return embeddings for the chosen ESM-2 model, using only the last layer by default.\"\"\"\n",
    "    try:\n",
    "        esm2, esm2_alphabet = getattr(esm.pretrained, pretrained_esm_model)()\n",
    "    except AttributeError:\n",
    "        print(f\"`pretrained_esm_model` must be one of {PRETRAINED_ESM_MODELS}\")\n",
    "    else:\n",
    "        if repr_layers is None:\n",
    "            repr_layers = [len(esm2.layers)]\n",
    "        print(\n",
    "            f\"Using pretrained model {pretrained_esm_model}, \"\n",
    "            f\"extracting representation layers {repr_layers}\"\n",
    "        )\n",
    "        esm2 = esm2.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            esm2.cuda()\n",
    "        esm2_batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "        sequences = get_sequences_from_parsed_prody(parsed_structures_prody)\n",
    "\n",
    "        esm2_embeddings = {}\n",
    "        with torch.no_grad():\n",
    "            for name, chains in tqdm(sequences.items()):\n",
    "                esm2_embeddings[name] = {}\n",
    "                for chain_id, seq in chains.items():\n",
    "                    (\n",
    "                        esm2_batch_labels,\n",
    "                        esm2_batch_strs,\n",
    "                        esm2_batch_tokens,\n",
    "                    ) = esm2_batch_converter([(f\"{name}, chain {chain_id}\", seq)])\n",
    "                    esm2_batch_tokens = esm2_batch_tokens.to(\n",
    "                        next(esm2.parameters()).device\n",
    "                    )\n",
    "                    # Extract per-residue representations (on CPU)\n",
    "\n",
    "                    results = esm2(\n",
    "                        esm2_batch_tokens,\n",
    "                        repr_layers=repr_layers,\n",
    "                        return_contacts=False,\n",
    "                    )\n",
    "                    token_representations = [\n",
    "                        results[\"representations\"][layer].cpu() for layer in repr_layers\n",
    "                    ]\n",
    "                    esm2_embeddings[name][chain_id] = token_representations\n",
    "\n",
    "    return esm2_embeddings\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def main(\n",
    "    structures_dir: Param(\"Directory containing structures in PDB format\", str),\n",
    "    output_dir: Param(\"Output directory for the embeddings\", str),\n",
    "    pretrained_esm_model: Param(\n",
    "        \"Pretrained ESM-2 model name\", str, default=\"esm2_t36_3B_UR50D\"\n",
    "    ),\n",
    "):\n",
    "    structures_dir = Path(structures_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    structures_paths = {\n",
    "        x.name.removesuffix(\".pdb\"): x for x in sorted(structures_dir.glob(\"*.pdb\"))\n",
    "    }\n",
    "    parsed_structures_prody = {\n",
    "        name: parsePDB(str(path)) for name, path in structures_paths.items()\n",
    "    }\n",
    "\n",
    "    esm2_embeddings = embeddings_from_parsed_prody(\n",
    "        parsed_structures_prody, pretrained_esm_model\n",
    "    )\n",
    "\n",
    "    for name, embeddings in esm2_embeddings.items():\n",
    "        with open(output_dir / f\"{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "\n",
    "    return esm2_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(embeddings_from_parsed_prody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
