{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import pandas as pd\n",
    "\n",
    "from prody import AtomGroup, parsePDB\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "from fastcore.script import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ca_coords_from_parsed_prody(parsed_structure_prody: AtomGroup):\n",
    "    \"\"\"Extract 3D coordinates of alpha carbons from a parsed ProDy structure.\"\"\"\n",
    "    coords = {}\n",
    "    for chain in parsed_structure_prody.getHierView():\n",
    "        chain_id = chain.getChid()\n",
    "        chain_ca = chain.select(\"name CA\")\n",
    "        chain_coords = chain_ca.getCoords()\n",
    "        assert len(chain_coords) == len(chain_ca.getSequence())\n",
    "        coords[chain_id] = chain_coords\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def create_raw_dataset(\n",
    "    structures_dir: Param(\"Directory containing structures in PDB format\", str),\n",
    "    esm_embeddings_dir: Param(\"Directory containing ESM embeddings\", str),\n",
    "    labels_path: Param(\"Path for target labels in text format\", str),\n",
    "    output_dir: Param(\"Output directory for the dataset\", str),\n",
    "):\n",
    "    structures_dir = Path(structures_dir)\n",
    "    esm_embeddings_dir = Path(esm_embeddings_dir)\n",
    "    labels_path = Path(labels_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir_raw = output_dir / \"raw\"\n",
    "    output_dir_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sorted_structure_paths = sorted(structures_dir.glob(\"*.pdb\"))\n",
    "\n",
    "    # Load target labels into a pandas dataframe\n",
    "    Y = pd.read_csv(\n",
    "        labels_path,\n",
    "        sep=\" \",\n",
    "        header=None,\n",
    "        names=[\"Structure\", \"Value\"],\n",
    "        index_col=\"Structure\",\n",
    "    ).squeeze()\n",
    "\n",
    "    raw_dataset = []\n",
    "    for idx, structure_path in enumerate(sorted_structure_paths):\n",
    "        raw_data = {\"idx\": torch.tensor([idx])}\n",
    "\n",
    "        name = structure_path.name.removesuffix(\".pdb\")\n",
    "        raw_data[\"name\"] = name\n",
    "\n",
    "        # Get 3D Ca coordinates\n",
    "        parsed_structure_prody = parsePDB(str(structure_path))\n",
    "        coords = get_ca_coords_from_parsed_prody(parsed_structure_prody)\n",
    "        raw_data[\"pos\"] = torch.cat(\n",
    "            [\n",
    "                torch.from_numpy(chain_coords).to(torch.float32)\n",
    "                for chain_coords in coords.values()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Get ESM embeddings\n",
    "        with open(esm_embeddings_dir / f\"{name}.pkl\", \"rb\") as f:\n",
    "            esm_embeddings = pickle.load(f)\n",
    "        raw_data[\"esm_embeddings\"] = torch.cat(\n",
    "            [esm_embeddings[chain_id][0][0][1:-1] for chain_id in coords]\n",
    "        )\n",
    "        assert len(raw_data[\"esm_embeddings\"]) == len(raw_data[\"pos\"])\n",
    "\n",
    "        raw_data[\"y\"] = torch.tensor([[Y[name]]]).to(torch.float32)\n",
    "\n",
    "        raw_dataset.append(raw_data)\n",
    "\n",
    "    torch.save(raw_dataset, output_dir_raw / \"data.pt\")\n",
    "\n",
    "    return raw_dataset\n",
    "\n",
    "\n",
    "def make_train_val_split_clustering_by_rmsd(rmsds_matrix, cutoff, train_frac=0.8):\n",
    "    Z = linkage(squareform(rmsds_matrix, checks=False), method=\"complete\")\n",
    "    labels = fcluster(Z, cutoff, criterion=\"distance\")\n",
    "    unique, inverse = np.unique(labels, return_inverse=True)\n",
    "    unique_shuffled = unique.copy()\n",
    "    np.random.shuffle(unique_shuffled)\n",
    "    train_labels, val_labels = np.split(\n",
    "        unique_shuffled, [int(train_frac * len(unique))]\n",
    "    )\n",
    "\n",
    "    train_idxs = []\n",
    "    val_idxs = []\n",
    "    for i, label in enumerate(inverse):\n",
    "        if np.isin(label, train_labels):\n",
    "            train_idxs.append(i)\n",
    "        else:\n",
    "            val_idxs.append(i)\n",
    "\n",
    "    return train_idxs, val_idxs\n",
    "\n",
    "\n",
    "class JClinicDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return \"data.pt\"\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"data.pt\"\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into a `Data` list\n",
    "        data_list = [\n",
    "            Data(**raw_dict)\n",
    "            for raw_dict in torch.load(Path(self.raw_dir) / self.raw_file_names)\n",
    "        ]\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(get_ca_coords_from_parsed_prody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
