{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models\n",
    "\n",
    "> Modified SchNet model to predict labels, from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/schnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import warnings\n",
    "from math import pi as PI\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Linear, ModuleList, Sequential\n",
    "\n",
    "from torch_geometric.data import Dataset, download_url, extract_zip\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.nn import MessagePassing, SumAggregation, radius_graph\n",
    "from torch_geometric.nn.resolver import aggregation_resolver as aggr_resolver\n",
    "from torch_geometric.typing import OptTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SchNet(torch.nn.Module):\n",
    "    r\"\"\"The continuous-filter convolutional neural network SchNet from the\n",
    "    `\"SchNet: A Continuous-filter Convolutional Neural Network for Modeling\n",
    "    Quantum Interactions\" <https://arxiv.org/abs/1706.08566>`_ paper that uses\n",
    "    the interactions blocks of the form\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\odot\n",
    "        h_{\\mathbf{\\Theta}} ( \\exp(-\\gamma(\\mathbf{e}_{j,i} - \\mathbf{\\mu}))),\n",
    "\n",
    "    here :math:`h_{\\mathbf{\\Theta}}` denotes an MLP and\n",
    "    :math:`\\mathbf{e}_{j,i}` denotes the interatomic distances between atoms.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using a pretrained SchNet variant, see\n",
    "        `examples/qm9_pretrained_schnet.py\n",
    "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        qm9_pretrained_schnet.py>`_.\n",
    "\n",
    "    Args:\n",
    "        esm_embedding_dim (int): Input dimension of ESM embeddings.\n",
    "        hidden_channels (int, optional): Hidden embedding size.\n",
    "            (default: :obj:`128`)\n",
    "        num_filters (int, optional): The number of filters to use.\n",
    "            (default: :obj:`128`)\n",
    "        num_interactions (int, optional): The number of interaction blocks.\n",
    "            (default: :obj:`6`)\n",
    "        num_gaussians (int, optional): The number of gaussians :math:`\\mu`.\n",
    "            (default: :obj:`50`)\n",
    "        interaction_graph (callable, optional): The function used to compute\n",
    "            the pairwise interaction graph and interatomic distances. If set to\n",
    "            :obj:`None`, will construct a graph based on :obj:`cutoff` and\n",
    "            :obj:`max_num_neighbors` properties.\n",
    "            If provided, this method takes in :obj:`pos` and :obj:`batch`\n",
    "            tensors and should return :obj:`(edge_index, edge_weight)` tensors.\n",
    "            (default :obj:`None`)\n",
    "        cutoff (float, optional): Cutoff distance for interatomic interactions.\n",
    "            (default: :obj:`10.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance.\n",
    "            (default: :obj:`32`)\n",
    "        readout (str, optional): Whether to apply :obj:`\"add\"` or :obj:`\"mean\"`\n",
    "            global aggregation. (default: :obj:`\"add\"`)\n",
    "        mean (float, optional): The mean of the property to predict.\n",
    "            (default: :obj:`None`)\n",
    "        std (float, optional): The standard deviation of the property to\n",
    "            predict. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        esm_embedding_dim: int,\n",
    "        hidden_channels: int = 128,\n",
    "        num_filters: int = 128,\n",
    "        num_interactions: int = 6,\n",
    "        num_gaussians: int = 50,\n",
    "        cutoff: float = 10.0,\n",
    "        interaction_graph: Optional[Callable] = None,\n",
    "        max_num_neighbors: int = 32,\n",
    "        readout: str = \"sum\",\n",
    "        mean: Optional[float] = None,\n",
    "        std: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm_embedding_dim = esm_embedding_dim\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_filters = num_filters\n",
    "        self.num_interactions = num_interactions\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.cutoff = cutoff\n",
    "        self.sum_aggr = SumAggregation()\n",
    "        self.readout = aggr_resolver(readout)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.scale = None\n",
    "\n",
    "        self.lin0 = Linear(esm_embedding_dim, hidden_channels)\n",
    "\n",
    "        self.interaction_graph = RadiusInteractionGraph(cutoff, max_num_neighbors)\n",
    "\n",
    "        self.distance_expansion = GaussianSmearing(0.0, cutoff, num_gaussians)\n",
    "\n",
    "        self.interactions = ModuleList()\n",
    "        for _ in range(num_interactions):\n",
    "            block = InteractionBlock(\n",
    "                hidden_channels, num_gaussians, num_filters, cutoff\n",
    "            )\n",
    "            self.interactions.append(block)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.act = ShiftedSoftplus()\n",
    "        self.lin2 = Linear(hidden_channels // 2, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        self.lin0.reset_parameters()\n",
    "        for interaction in self.interactions:\n",
    "            interaction.reset_parameters()\n",
    "        torch.nn.init.xavier_uniform_(self.lin1.weight)\n",
    "        self.lin1.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.lin2.weight)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self, esm_embeddings: Tensor, pos: Tensor, batch: OptTensor = None\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            esm_embeddings (torch.Tensor): ESM embeddings of each residue\n",
    "                with shape :obj:`[num_atoms]`.\n",
    "            pos (torch.Tensor): Coordinates of each atom with shape\n",
    "                :obj:`[num_atoms, 3]`.\n",
    "            batch (torch.Tensor, optional): Batch indices assigning each atom\n",
    "                to a separate molecule with shape :obj:`[num_atoms]`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        batch = torch.zeros_like(esm_embeddings) if batch is None else batch\n",
    "\n",
    "        h = self.lin0(esm_embeddings)\n",
    "        edge_index, edge_weight = self.interaction_graph(pos, batch)\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        for interaction in self.interactions:\n",
    "            h = h + interaction(h, edge_index, edge_weight, edge_attr)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = self.act(h)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            h = h * self.std + self.mean\n",
    "\n",
    "        out = self.readout(h, batch, dim=0)\n",
    "\n",
    "        if self.scale is not None:\n",
    "            out = self.scale * out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"esm_embedding_dim={self.esm_embedding_dim}, \"\n",
    "            f\"hidden_channels={self.hidden_channels}, \"\n",
    "            f\"num_filters={self.num_filters}, \"\n",
    "            f\"num_interactions={self.num_interactions}, \"\n",
    "            f\"num_gaussians={self.num_gaussians}, \"\n",
    "            f\"cutoff={self.cutoff})\"\n",
    "        )\n",
    "\n",
    "\n",
    "class RadiusInteractionGraph(torch.nn.Module):\n",
    "    r\"\"\"Creates edges based on atom positions :obj:`pos` to all points within\n",
    "    the cutoff distance.\n",
    "\n",
    "    Args:\n",
    "        cutoff (float, optional): Cutoff distance for interatomic interactions.\n",
    "            (default: :obj:`10.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance with the\n",
    "            default interaction graph method.\n",
    "            (default: :obj:`32`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cutoff: float = 10.0, max_num_neighbors: int = 32):\n",
    "        super().__init__()\n",
    "        self.cutoff = cutoff\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "\n",
    "    def forward(self, pos: Tensor, batch: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            pos (Tensor): Coordinates of each atom.\n",
    "            batch (LongTensor, optional): Batch indices assigning each atom to\n",
    "                a separate molecule.\n",
    "\n",
    "        :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "        edge_index = radius_graph(\n",
    "            pos, r=self.cutoff, batch=batch, max_num_neighbors=self.max_num_neighbors\n",
    "        )\n",
    "        row, col = edge_index\n",
    "        edge_weight = (pos[row] - pos[col]).norm(dim=-1)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "\n",
    "class InteractionBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_channels: int, num_gaussians: int, num_filters: int, cutoff: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mlp = Sequential(\n",
    "            Linear(num_gaussians, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "            Linear(num_filters, num_filters),\n",
    "        )\n",
    "        self.conv = CFConv(\n",
    "            hidden_channels, hidden_channels, num_filters, self.mlp, cutoff\n",
    "        )\n",
    "        self.act = ShiftedSoftplus()\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.mlp[0].weight)\n",
    "        self.mlp[0].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.mlp[2].weight)\n",
    "        self.mlp[2].bias.data.fill_(0)\n",
    "        self.conv.reset_parameters()\n",
    "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Tensor, edge_weight: Tensor, edge_attr: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = self.conv(x, edge_index, edge_weight, edge_attr)\n",
    "        x = self.act(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CFConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        num_filters: int,\n",
    "        nn: Sequential,\n",
    "        cutoff: float,\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.lin1 = Linear(in_channels, num_filters, bias=False)\n",
    "        self.lin2 = Linear(num_filters, out_channels)\n",
    "        self.nn = nn\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.lin1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.lin2.weight)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Tensor, edge_weight: Tensor, edge_attr: Tensor\n",
    "    ) -> Tensor:\n",
    "        C = 0.5 * (torch.cos(edge_weight * PI / self.cutoff) + 1.0)\n",
    "        W = self.nn(edge_attr) * C.view(-1, 1)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.propagate(edge_index, x=x, W=W)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j: Tensor, W: Tensor) -> Tensor:\n",
    "        return x_j * W\n",
    "\n",
    "\n",
    "class GaussianSmearing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start: float = 0.0,\n",
    "        stop: float = 5.0,\n",
    "        num_gaussians: int = 50,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        offset = torch.linspace(start, stop, num_gaussians)\n",
    "        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2\n",
    "        self.register_buffer(\"offset\", offset)\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:\n",
    "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))\n",
    "\n",
    "\n",
    "class ShiftedSoftplus(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.softplus(x) - self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
